<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Spark-SQL原理 | WeiJia_Rao</title><meta name="keywords" content="Spark SQL"><meta name="author" content="WeiJia Rao"><meta name="copyright" content="WeiJia Rao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="SparkSQL中的join 数据分析中将两个数据集进行 Join 操作是很常见的场景。在 Spark 的物理计划阶段，Spark 的 Join Selection 类会根据 Join hints 策略、Join 表的大小、 Join 是等值Join 还是不等值以及参与 Join 的 key 是否可以排序等条件来选择最终的 Join 策略，最后 Spark 会利用选择好的 Join 策略执行最终的">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark-SQL原理">
<meta property="og:url" content="https://raoweijiapng.gitee.io/posts/1437152698/index.html">
<meta property="og:site_name" content="WeiJia_Rao">
<meta property="og:description" content="SparkSQL中的join 数据分析中将两个数据集进行 Join 操作是很常见的场景。在 Spark 的物理计划阶段，Spark 的 Join Selection 类会根据 Join hints 策略、Join 表的大小、 Join 是等值Join 还是不等值以及参与 Join 的 key 是否可以排序等条件来选择最终的 Join 策略，最后 Spark 会利用选择好的 Join 策略执行最终的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raoweijiapng.gitee.io/img/8.jpg">
<meta property="article:published_time" content="2022-09-02T11:19:59.000Z">
<meta property="article:modified_time" content="2022-09-03T03:12:00.000Z">
<meta property="article:author" content="WeiJia Rao">
<meta property="article:tag" content="Spark SQL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raoweijiapng.gitee.io/img/8.jpg"><link rel="shortcut icon" href="/img/networkPhoto.jpg"><link rel="canonical" href="https://raoweijiapng.gitee.io/posts/1437152698/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?a9e49a68498fd088e63e2fe8907ca570";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark-SQL原理',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-03 11:12:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/authorPhoto.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">419</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">70</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">59</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/8.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">WeiJia_Rao</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark-SQL原理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-02T11:19:59.000Z" title="发表于 2022-09-02 19:19:59">2022-09-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-03T03:12:00.000Z" title="更新于 2022-09-03 11:12:00">2022-09-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark-SQL原理"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="SparkSQL中的join">SparkSQL中的join</h2>
<p>数据分析中将两个数据集进行 Join 操作是很常见的场景。在 Spark 的物理计划阶段，Spark 的 Join Selection 类会根据 Join hints 策略、Join 表的大小、 Join 是等值Join 还是不等值以及参与 Join 的 key 是否可以排序等条件来选择最终的 Join 策略，最后 Spark 会利用选择好的 Join 策略执行最终的计算。当前 Spark 一共支持五种 Join 策略：</p>
<ul>
<li>
<p><strong>Broadcast hash join (BHJ)</strong></p>
</li>
<li>
<p>Shuffle hash join（SHJ）</p>
</li>
<li>
<p><strong>Shuffle sort merge join (SMJ)</strong></p>
</li>
<li>
<p>Shuffle-and-replicate nested loop join，又称笛卡尔积（Cartesian product join)</p>
</li>
<li>
<p>Broadcast nested loop join (BNLJ)</p>
</li>
</ul>
<p>其中 BHJ 和 SMJ 这两种 Join 策略是我们运行 Spark 作业最常见的。</p>
<p>JoinSelection 会先根据 Join 的 Key 为等值 Join 来选择 Broadcast hash join、Shuffle hash join 以 及 Shuffle sort merge join 中的一个；如果 Join 的 Key 为不等值 Join 或者没有指定 Join 条件，则会选择 Broadcast nested loop join 或 Shuffle-and-replicate nested loop join。</p>
<p>不同的 Join 策略在执行上效率差别很大，了解每种 Join 策略的执行过程和适用条件是很有必要的。</p>
<h3 id="Broadcast-Hash-Join">Broadcast Hash Join</h3>
<p>Broadcast Hash Join 的实现是将小表的数据广播到 Spark 所有的 Executor 端，这个广播过程和我们自己去广播数据没什么区别：</p>
<ul>
<li>
<p>利用 collect 算子将小表的数据从 Executor 端拉到 Driver 端</p>
</li>
<li>
<p>在 Driver 端调用 sparkContext.broadcast 广播到所有 Executor 端</p>
</li>
<li>
<p>在 Executor 端使用广播的数据与大表进行 Join 操作（实际上是执行map操作）</p>
</li>
</ul>
<p>这种 Join 策略避免了 Shuffle 操作。一般而言，Broadcast Hash Join 会比其他 Join 策略执行的要快。</p>
  <img src="/posts/1437152698/1.jpg" class="post-image">
  <br>
<p>使用这种 Join 策略必须满足以下条件：</p>
<ul>
<li>
<p>小表的数据必须很小，可以通过 spark.sql.autoBroadcastJoinThreshold 参数来配置，默认是 10MB</p>
</li>
<li>
<p>如果内存比较大，可以将阈值适当加大</p>
</li>
<li>
<p>将 spark.sql.autoBroadcastJoinThreshold 参数设置为 -1，可以关闭这种连接方式</p>
</li>
<li>
<p>只能用于等值 Join，不要求参与 Join 的 keys 可排序</p>
</li>
</ul>
<h3 id="Shuffle-Hash-Join">Shuffle Hash Join</h3>
<p>当表中的数据比较大，又不适合使用广播，这个时候就可以考虑使用 Shuffle Hash Join。</p>
<p>Shuffle Hash Join 同样是在大表和小表进行 Join 的时候选择的一种策略。它的计算思想是：把大表和小表按照相同的分区算法和分区数进行分区（根据参与 Join 的 keys 进行分区），这样就保证了 hash 值一样的数据都分发到同一个分区中，然后在同一个 Executor 中两张表 hash 值一样的分区就可以在本地进行 hash Join 了。在进行 Join 之前，还会对小表的分区构建 Hash Map。Shuffle hash join 利用了分治思想，把大问题拆解成小问题去解决。</p>
  <img src="/posts/1437152698/2.jpg" class="post-image">
  <br>
<p>要启用 Shuffle Hash Join 必须满足以下条件：</p>
<ul>
<li>
<p>仅支持等值 Join，不要求参与 Join 的 Keys 可排序</p>
</li>
<li>
<p>spark.sql.join.preferSortMergeJoin 参数必须设置为 false，参数是从 Spark 2.0.0 版本引入的，默认值为 true，也就是默认情况下选择 Sort Merge Join</p>
</li>
<li>
<p>小表的大小（plan.stats.sizeInBytes）必须小于 spark.sql.autoBroadcastJoinThreshold * spark.sql.shuffle.partitions（默认值200）</p>
</li>
<li>
<p>而且小表大小（stats.sizeInBytes）的三倍必须小于等于大表的大小（stats.sizeInBytes），也就是 a.stats.sizeInBytes * 3 &lt; = b.stats.sizeInBytes</p>
</li>
</ul>
<h3 id="Shuffle-Sort-Merge-Join">Shuffle Sort Merge Join</h3>
<p>前面两种 Join 策略对表的大小都有条件的，如果参与 Join 的表都很大，这时候就得考虑用 Shuffle Sort Merge Join 了。</p>
<p>Shuffle Sort Merge Join 的实现思想：</p>
<ul>
<li>
<p>将两张表按照 join key 进行shuffle，保证join key值相同的记录会被分在相应的分区</p>
</li>
<li>
<p>对每个分区内的数据进行排序</p>
</li>
<li>
<p>排序后再对相应的分区内的记录进行连接</p>
</li>
</ul>
<p>无论分区有多大，Sort Merge Join都不用把一侧的数据全部加载到内存中，而是即用即丢；因为两个序列都有序。从头遍历，碰到key相同的就输出，如果不同，左边小就继续取左边，反之取右边。从而大大提高了大数据量下sql join的稳定性。</p>
  <img src="/posts/1437152698/3.jpg" class="post-image">
  <br>
<p>要启用 Shuffle Sort Merge Join 必须满足以下条件：</p>
<ul>
<li>仅支持等值 Join，并且要求参与 Join 的 Keys 可排序</li>
</ul>
<h3 id="Cartesian-product-join">Cartesian product join</h3>
<p>如果 Spark 中两张参与 Join 的表没指定连接条件，那么会产生 Cartesian product join，这个 Join 得到的结果其实就是两张表行数的乘积。</p>
<h3 id="Broadcast-nested-loop-join">Broadcast nested loop join</h3>
<p>可以把 Broadcast nested loop join 的执行看做下面的计算：</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for record_1 in relation_1:</span><br><span class="line">  for record_2 in relation_2:</span><br><span class="line">    # join condition is executed</span><br></pre></td></tr></table></figure>
<p>可以看出 Broadcast nested loop join 在某些情况会对某张表重复扫描多次，效率非常低下。从名字可以看出，这种 join 会根据相关条件对小表进行广播，以减少表的扫描次数。</p>
<p>Broadcast nested loop join 支持等值和不等值 Join，支持所有的 Join 类型。</p>
<h2 id="SQL解析过程">SQL解析过程</h2>
<p>Spark SQL 可以说是 Spark 中的精华部分。原来基于 RDD 构建大数据计算任务，重心在向 DataSet 转移，原来基于 RDD 写的代码也在迁移。使用 Spark SQL 编码好处是非常大的，尤其是在性能方面，有很大提升。Spark SQL 中各种内嵌的性能优化比写 RDD 遵守各种最佳实践更靠谱的，尤其对新手来说。如先 filter 操作再 map 操作，Spark SQL 中会自动进行谓词下推；Spark SQL中会自动使用 broadcast join 来广播小表，把 shuffle join 转化为 map join 等等。</p>
<p>Spark SQL对SQL语句的处理和关系型数据库类似，即词法/语法解析、绑定、优化、执行。Spark SQL会先将SQL语句解析成一棵树，然后使用规则(Rule)对Tree进行绑定、优化等处理过程。Spark SQL由Core、Catalyst、Hive、Hive-ThriftServer四部分构成：</p>
<ul>
<li>
<p>Core: 负责处理数据的输入和输出，如获取数据，查询结果输出成DataFrame等</p>
</li>
<li>
<p><strong>Catalyst: 负责处理整个查询过程，包括解析、绑定、优化等</strong></p>
</li>
<li>
<p>Hive: 负责对Hive数据进行处理</p>
</li>
<li>
<p>Hive-ThriftServer: 主要用于对Hive的访问</p>
</li>
</ul>
  <img src="/posts/1437152698/4.jpg" class="post-image">
  <br>
  <img src="/posts/1437152698/5.jpg" class="post-image">
  <br>
<p>Spark SQL的代码复杂度是问题的本质复杂度带来的，Spark SQL中的 Catalyst 框架大部分逻辑是在一个 Tree 类型的数据结构上做各种折腾，基于 Scala 来实现还是很优雅的，Scala 的偏函数和强大的 Case 正则匹配，让整个代码看起来非常优雅。</p>
<p>SparkSession 是编写 Spark 应用代码的入口，启动一个 spark-shell 会提供给你一个创建 SparkSession， 这个对象是整个 Spark 应用的起始点。以下是 SparkSession 的一些重要的变量和方法：</p>
<table>
<thead>
<tr>
<th style="text-align:left">类</th>
<th style="text-align:left">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">catalog</td>
<td style="text-align:left">通过对这个类可以操作元数据，对数据库、表、函数进行增删改查，内部使用SessionCatalog完成具体操作</td>
</tr>
<tr>
<td style="text-align:left">table</td>
<td style="text-align:left">把一个table或view包装为一个DataFrame进行后续操作</td>
</tr>
<tr>
<td style="text-align:left">emptyDataset / emptyDataFrame</td>
<td style="text-align:left">创建空的Dataset 或 DataFrame</td>
</tr>
<tr>
<td style="text-align:left">sql</td>
<td style="text-align:left">执行sql，返回一个DataFrame</td>
</tr>
<tr>
<td style="text-align:left">read 或 readStream</td>
<td style="text-align:left">获取数据读取器，读取各种格式的数据</td>
</tr>
<tr>
<td style="text-align:left"><strong>sessionState</strong></td>
<td style="text-align:left">维护了当前session使用的所有状态数据；还包括SQL解析器、分析器、优化器等</td>
</tr>
</tbody>
</table>
  <img src="/posts/1437152698/6.jpg" class="post-image">
  <br>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.lagou.sparksql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Plan</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">&quot;Demo1&quot;</span>)</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    spark.sparkContext.setLogLevel(<span class="string">&quot;warn&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="type">Seq</span>((<span class="number">0</span>, <span class="string">&quot;zhansan&quot;</span>, <span class="number">10</span>),</span><br><span class="line">      (<span class="number">1</span>, <span class="string">&quot;lisi&quot;</span>, <span class="number">11</span>),</span><br><span class="line">      (<span class="number">2</span>, <span class="string">&quot;wangwu&quot;</span>, <span class="number">12</span>)).toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).createOrReplaceTempView(<span class="string">&quot;stu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="type">Seq</span>((<span class="number">0</span>, <span class="string">&quot;chinese&quot;</span>, <span class="number">80</span>), (<span class="number">0</span>, <span class="string">&quot;math&quot;</span>, <span class="number">100</span>), (<span class="number">0</span>, <span class="string">&quot;english&quot;</span>, <span class="number">98</span>),</span><br><span class="line">      (<span class="number">1</span>, <span class="string">&quot;chinese&quot;</span>, <span class="number">86</span>), (<span class="number">1</span>, <span class="string">&quot;math&quot;</span>, <span class="number">97</span>), (<span class="number">1</span>, <span class="string">&quot;english&quot;</span>, <span class="number">90</span>),</span><br><span class="line">      (<span class="number">2</span>, <span class="string">&quot;chinese&quot;</span>, <span class="number">90</span>), (<span class="number">2</span>, <span class="string">&quot;math&quot;</span>, <span class="number">94</span>), (<span class="number">2</span>, <span class="string">&quot;english&quot;</span>, <span class="number">88</span>)</span><br><span class="line">    ).toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;subject&quot;</span>, <span class="string">&quot;score&quot;</span>).createOrReplaceTempView(<span class="string">&quot;score&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |select sum(v), name</span></span><br><span class="line"><span class="string">        |  from (select stu.id, 100 + 10 + score.score as v, name</span></span><br><span class="line"><span class="string">        |          from stu join score</span></span><br><span class="line"><span class="string">        |          where stu.id = score.id and stu.age &gt;= 11) tmp</span></span><br><span class="line"><span class="string">        |group by name</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">    println(df.queryExecution)</span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>queryExecution 就是整个执行计划的执行引擎，里面有执行过程中各个中间过程变量，整个执行流程如下：</p>
  <img src="/posts/1437152698/7.jpg" class="post-image">
  <br>
<p>上面例子中的 SQL 语句经过 Parser 解析后就会变成一个抽象语法树，对应解析后的逻辑计划 AST 为：</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">== Parsed Logical Plan ==</span><br><span class="line">&#x27;Aggregate [&#x27;name], [unresolvedalias(&#x27;sum(&#x27;v), None), &#x27;name]</span><br><span class="line">+- &#x27;SubqueryAlias `tmp`</span><br><span class="line">   +- &#x27;Project [&#x27;stu.id, ((100 + 10) + &#x27;score.score) AS v#26, &#x27;name]</span><br><span class="line">      +- &#x27;Filter ((&#x27;stu.id = &#x27;score.id) &amp;&amp; (&#x27;stu.age &gt;= 11))</span><br><span class="line">         +- &#x27;Join Inner</span><br><span class="line">            :- &#x27;UnresolvedRelation `stu`</span><br><span class="line">            +- &#x27;UnresolvedRelation `score`</span><br></pre></td></tr></table></figure>
<p>备注：在执行计划中 Project/Projection 代表的意思是投影</p>
<p>选投连三种最基本的操作</p>
  <img src="/posts/1437152698/8.jpg" class="post-image">
  <br>
<p>其中过滤条件变为了 Filter 节点，这个节点是 UnaryNode(一元节点) 类型， 只有一个孩子。两个表中的数据变为了 UnresolvedRelation 节点，节点类型为 LeafNode，即叶子节点， JOIN 操作为节点， 这个是一个 BinaryNode 节点，有两个孩子。</p>
<p>以上节点都是 LogicalPlan 类型的， 可以理解为进行各种操作的 Operator，SparkSQL 对各种操作定义了各种 Operator。</p>
  <img src="/posts/1437152698/9.jpg" class="post-image">
  <br>
<p>这些 operator 组成的抽象语法树就是整个 Catatyst 优化的基础，Catatyst 优化器会在这个树上面进行各种折腾，把树上面的节点挪来挪去来进行优化。</p>
<p>经过 Parser 有了抽象语法树，但是并不知道 score，sum 这些东西是啥，所以就需要 analyer 来定位。</p>
<p>analyzer 会把 AST 上所有 Unresolved 的东西都转变为 resolved 状态，SparkSQL有很多resolve 规则：</p>
<ul>
<li>
<p>ResolverRelations。解析表（列）的基本类型等信息</p>
</li>
<li>
<p>ResolveFuncions。解析出来函数的基本信息</p>
</li>
<li>
<p>ResolveReferences。解析引用，通常是解析列名</p>
</li>
</ul>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">== Analyzed Logical Plan ==</span><br><span class="line">sum(v): bigint, name: string</span><br><span class="line">Aggregate [name#8], [sum(cast(v#26 as bigint)) AS sum(v)#28L, name#8]</span><br><span class="line">+- SubqueryAlias `tmp`</span><br><span class="line">   +- Project [id#7, ((100 + 10) + score#22) AS v#26, name#8]</span><br><span class="line">      +- Filter ((id#7 = id#20) &amp;&amp; (age#9 &gt;= 11))</span><br><span class="line">         +- Join Inner</span><br><span class="line">            :- SubqueryAlias `stu`</span><br><span class="line">            :  +- Project [_1#3 AS id#7, _2#4 AS name#8, _3#5 AS age#9]</span><br><span class="line">            :     +- LocalRelation [_1#3, _2#4, _3#5]</span><br><span class="line">            +- SubqueryAlias `score`</span><br><span class="line">               +- Project [_1#16 AS id#20, _2#17 AS subject#21, _3#18 AS score#22]</span><br><span class="line">                  +- LocalRelation [_1#16, _2#17, _3#18]</span><br></pre></td></tr></table></figure>
  <img src="/posts/1437152698/10.jpg" class="post-image">
  <br>
<p>下面要进行逻辑优化了，常见的逻辑优化有：</p>
  <img src="/posts/1437152698/11.jpg" class="post-image">
  <br>
  <img src="/posts/1437152698/12.jpg" class="post-image">
  <br>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">== Optimized Logical Plan ==</span><br><span class="line">Aggregate [name#8], [sum(cast(v#26 as bigint)) AS sum(v)#28L, name#8]</span><br><span class="line">+- Project [(110 + score#22) AS v#26, name#8]</span><br><span class="line">   +- Join Inner, (id#7 = id#20)</span><br><span class="line">      :- LocalRelation [id#7, name#8]</span><br><span class="line">      +- LocalRelation [id#20, score#22]</span><br></pre></td></tr></table></figure>
<p>这里用到的优化有：谓词下推(Push Down Predicate)、常量折叠(Constant Folding)、字段裁剪(Columning Pruning)</p>
  <img src="/posts/1437152698/13.jpg" class="post-image">
  <br>
<p>做完逻辑优化，还需要先转换为物理执行计划，将逻辑上可行的执行计划变为 Spark 可以真正执行的计划：</p>
  <img src="/posts/1437152698/14.jpg" class="post-image">
  <br>
<p>SparkSQL 把逻辑节点转换为了相应的物理节点， 比如 Join 算子，Spark 根据不同场景为该算子制定了不同的算法策略。</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">== Physical Plan ==</span><br><span class="line">*(2) HashAggregate(keys=[name#8], functions=[sum(cast(v#26 as bigint))], output=[sum(v)#28L, name#8])</span><br><span class="line">+- Exchange hashpartitioning(name#8, 200)</span><br><span class="line">   +- *(1) HashAggregate(keys=[name#8], functions=[partial_sum(cast(v#26 as bigint))], output=[name#8, sum#32L])</span><br><span class="line">      +- *(1) Project [(110 + score#22) AS v#26, name#8]</span><br><span class="line">         +- *(1) BroadcastHashJoin [id#7], [id#20], Inner, BuildLeft</span><br><span class="line">            :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))</span><br><span class="line">            :  +- LocalTableScan [id#7, name#8]</span><br><span class="line">            +- LocalTableScan [id#20, score#22]</span><br></pre></td></tr></table></figure>
<p>数据在一个一个的 plan 中流转，然后每个 plan 里面表达式都会对数据进行处理，就相当于经过了一个个小函数的调用处理，这里面有大量的函数调用开销。是不是可以把这些小函数内联一下，当成一个大函数，WholeStageCodegen 就是干这事的。可以看到最终执行计划每个节点前面有个 * 号，说明整段代码生成被启用，Project、BroadcastHashJoin、HashAggregate 这一段都启用了整段代码生成，级联为了大函数。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://raoweijiapng.gitee.io">WeiJia Rao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://raoweijiapng.gitee.io/posts/1437152698/">https://raoweijiapng.gitee.io/posts/1437152698/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://raoweijiapng.gitee.io" target="_blank">WeiJia_Rao</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark-SQL/">Spark SQL</a></div><div class="post_share"><div class="social-share" data-image="/img/8.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/3939076516/"><img class="prev-cover" src="/img/1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Spark-SQL编程之UDF和UDAF以及访问Hive</div></div></a></div><div class="next-post pull-right"><a href="/posts/1568661967/"><img class="next-cover" src="/img/6.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Spark-Streaming概述</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/1443186265/" title="Spark-SQL概述"><img class="cover" src="/img/7.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-31</div><div class="title">Spark-SQL概述</div></div></a></div><div><a href="/posts/2898344764/" title="Spark-SQL编程之Action操作与Transformation操作"><img class="cover" src="/img/8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-31</div><div class="title">Spark-SQL编程之Action操作与Transformation操作</div></div></a></div><div><a href="/posts/3618042008/" title="Spark-SQL编程之DataFrame和Dataset的创建"><img class="cover" src="/img/6.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-31</div><div class="title">Spark-SQL编程之DataFrame和Dataset的创建</div></div></a></div><div><a href="/posts/298274716/" title="Spark-SQL编程之SQL语句与输入输出"><img class="cover" src="/img/1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-01</div><div class="title">Spark-SQL编程之SQL语句与输入输出</div></div></a></div><div><a href="/posts/3939076516/" title="Spark-SQL编程之UDF和UDAF以及访问Hive"><img class="cover" src="/img/1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-01</div><div class="title">Spark-SQL编程之UDF和UDAF以及访问Hive</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/authorPhoto.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">WeiJia Rao</div><div class="author-info__description">饶唯甲的个人博客网站,用于记录平时的学习笔记并展示。努力学习吧,少年!</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">419</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">70</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">59</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://gitee.com/raoweijiapng"><i></i><span>My Gitee</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="mailto:raoweijia@outlook.com" target="_blank" title="邮箱"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://space.bilibili.com/217971296?spm_id_from=333.1007.0.0" target="_blank" title="哔哩哔哩"><i class="fa-brands fa-bilibili"></i></a><a class="social-icon" href="https://weibo.com/p/1005057628848053" target="_blank" title="微博"><i class="fa-brands fa-weibo"></i></a><a class="social-icon" href="https://www.zhihu.com/people/kan-kan-ni-66-95" target="_blank" title="知乎"><i class="fa-brands fa-zhihu"></i></a><a class="social-icon" href="https://github.com/raoweijiapng" target="_blank" title="My-GitHub"><i class="fa-brands fa-github"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#SparkSQL%E4%B8%AD%E7%9A%84join"><span class="toc-number">1.</span> <span class="toc-text">SparkSQL中的join</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Broadcast-Hash-Join"><span class="toc-number">1.1.</span> <span class="toc-text">Broadcast Hash Join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Shuffle-Hash-Join"><span class="toc-number">1.2.</span> <span class="toc-text">Shuffle Hash Join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Shuffle-Sort-Merge-Join"><span class="toc-number">1.3.</span> <span class="toc-text">Shuffle Sort Merge Join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cartesian-product-join"><span class="toc-number">1.4.</span> <span class="toc-text">Cartesian product join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Broadcast-nested-loop-join"><span class="toc-number">1.5.</span> <span class="toc-text">Broadcast nested loop join</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SQL%E8%A7%A3%E6%9E%90%E8%BF%87%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">SQL解析过程</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/3452200185/" title="第五章-信息系统工程">第五章-信息系统工程</a><time datetime="2023-08-15T09:38:42.000Z" title="发表于 2023-08-15 17:38:42">2023-08-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/2814949322/" title="第四章-信息系统管理">第四章-信息系统管理</a><time datetime="2023-08-14T01:17:33.000Z" title="发表于 2023-08-14 09:17:33">2023-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/428293102/" title="第三章-信息系统治理">第三章-信息系统治理</a><time datetime="2023-08-10T09:13:22.000Z" title="发表于 2023-08-10 17:13:22">2023-08-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/1420224338/" title="第二章-信息技术发展">第二章-信息技术发展</a><time datetime="2023-08-07T07:54:15.000Z" title="发表于 2023-08-07 15:54:15">2023-08-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/1809478863/" title="第一章-信息化发展">第一章-信息化发展</a><time datetime="2023-08-01T11:53:53.000Z" title="发表于 2023-08-01 19:53:53">2023-08-01</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/footer_img.png')"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2023 By WeiJia Rao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://raoweijiapng.gitee.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><div class="aplayer no-destroy" data-id="60198" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="false"> </div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script>(function(d, w, c) {
    w.ChatraID = '5QYmoz7m5kBKqo6Hi';
    var s = d.createElement('script');
    w[c] = w[c] || function() {
        (w[c].q = w[c].q || []).push(arguments);
    };
    s.async = true;
    s.src = 'https://call.chatra.io/chatra.js';
    if (d.head) d.head.appendChild(s);
})(document, window, 'Chatra');

if (false) {
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      Chatra('openChat')
    });
  }
  chatBtnFn()
} else {
  if (false) {
    function chatBtnHide () {
      Chatra('hide')
    }
    function chatBtnShow () {
      Chatra('show')
    }
  }
}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>