<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>RDD编程之Key-Value操作与输入与输出 | WeiJia_Rao</title><meta name="keywords" content="RDD,Spark Core"><meta name="author" content="WeiJia Rao"><meta name="copyright" content="WeiJia Rao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Key-Value RDD操作 RDD整体上分为 Value 类型和 Key-Value 类型。 前面介绍的是 Value 类型的RDD的操作，实际使用更多的是 key-value 类型的RDD，也称为 PairRDD。 Value 类型RDD的操作基本集中在 RDD.scala 中； key-value 类型的RDD操作集中在 PairRDDFunctions.scala 中；       前面">
<meta property="og:type" content="article">
<meta property="og:title" content="RDD编程之Key-Value操作与输入与输出">
<meta property="og:url" content="https://raoweijiapng.github.io/posts/1276889862/index.html">
<meta property="og:site_name" content="WeiJia_Rao">
<meta property="og:description" content="Key-Value RDD操作 RDD整体上分为 Value 类型和 Key-Value 类型。 前面介绍的是 Value 类型的RDD的操作，实际使用更多的是 key-value 类型的RDD，也称为 PairRDD。 Value 类型RDD的操作基本集中在 RDD.scala 中； key-value 类型的RDD操作集中在 PairRDDFunctions.scala 中；       前面">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raoweijiapng.github.io/img/1.jpg">
<meta property="article:published_time" content="2022-08-26T02:07:45.000Z">
<meta property="article:modified_time" content="2022-09-03T03:16:14.000Z">
<meta property="article:author" content="WeiJia Rao">
<meta property="article:tag" content="RDD">
<meta property="article:tag" content="Spark Core">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raoweijiapng.github.io/img/1.jpg"><link rel="shortcut icon" href="/img/networkPhoto.jpg"><link rel="canonical" href="https://raoweijiapng.github.io/posts/1276889862/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?a9e49a68498fd088e63e2fe8907ca570";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'RDD编程之Key-Value操作与输入与输出',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-03 11:16:14'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/authorPhoto.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">422</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">70</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">59</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/1.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">WeiJia_Rao</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">RDD编程之Key-Value操作与输入与输出</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-08-26T02:07:45.000Z" title="发表于 2022-08-26 10:07:45">2022-08-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-03T03:16:14.000Z" title="更新于 2022-09-03 11:16:14">2022-09-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="RDD编程之Key-Value操作与输入与输出"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Key-Value-RDD操作">Key-Value RDD操作</h2>
<p>RDD整体上分为 Value 类型和 Key-Value 类型。</p>
<p>前面介绍的是 Value 类型的RDD的操作，实际使用更多的是 key-value 类型的RDD，也称为 PairRDD。</p>
<p>Value 类型RDD的操作基本集中在 RDD.scala 中；</p>
<p>key-value 类型的RDD操作集中在 PairRDDFunctions.scala 中；</p>
  <img src="/posts/1276889862/1.jpg" class="post-image">
  <br>
<p>前面介绍的大多数算子对 Pair RDD 都是有效的。Pair RDD还有属于自己的Transformation、Action 算子；</p>
<h3 id="创建Pair-RDD">创建Pair RDD</h3>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val arr = (1 to 10).toArray</span><br><span class="line">arr: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</span><br><span class="line"></span><br><span class="line">scala&gt; val arr1 = arr.map(x =&gt; (x, x*10, x*100))</span><br><span class="line">arr1: Array[(Int, Int, Int)] = Array((1,10,100), (2,20,200), (3,30,300), (4,40,400), (5,50,500), (6,60,600), (7,70,700), (8,80,800), (9,90,900), (10,100,1000))</span><br><span class="line"></span><br><span class="line">// rdd1 不是 Pair RDD</span><br><span class="line">scala&gt; val rdd1 = sc.makeRDD(arr1)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(Int, Int, Int)] = ParallelCollectionRDD[10] at makeRDD at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.collectAsMap</span><br><span class="line">&lt;console&gt;:26: error: value collectAsMap is not a member of org.apache.spark.rdd.RDD[(Int, Int, Int)]</span><br><span class="line">       rdd1.collectAsMap</span><br><span class="line">            ^</span><br><span class="line"></span><br><span class="line">// rdd2 是 Pair RDD, 必须是key-value形式</span><br><span class="line">scala&gt; val arr2 = arr.map(x =&gt; (x, (x*10, x*100)))</span><br><span class="line">arr2: Array[(Int, (Int, Int))] = Array((1,(10,100)), (2,(20,200)), (3,(30,300)), (4,(40,400)), (5,(50,500)), (6,(60,600)), (7,(70,700)), (8,(80,800)), (9,(90,900)), (10,(100,1000)))</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = sc.makeRDD(arr2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = ParallelCollectionRDD[11] at makeRDD at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.collectAsMap</span><br><span class="line">res19: scala.collection.Map[Int,(Int, Int)] = Map(8 -&gt; (80,800), 2 -&gt; (20,200), 5 -&gt; (50,500), 4 -&gt; (40,400), 7 -&gt; (70,700), 10 -&gt; (100,1000), 1 -&gt; (10,100), 9 -&gt; (90,900), 3 -&gt; (30,300), 6 -&gt; (60,600))</span><br></pre></td></tr></table></figure>
<h3 id="Transformation操作">Transformation操作</h3>
<h4 id="类似-map-操作">类似 map 操作</h4>
<p><strong>mapValues / flatMapValues / keys / values，这些操作都可以使用 map 操作实现，是简化操作。</strong></p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(List((1,2),(3,4),(5,6)))</span><br><span class="line">a: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; a.collect</span><br><span class="line">res23: Array[(Int, Int)] = Array((1,2), (3,4), (5,6))</span><br><span class="line"></span><br><span class="line">// 使用 mapValues 更简洁,只修改value值</span><br><span class="line">scala&gt; val b = a.mapValues(x=&gt;1 to x)</span><br><span class="line">b: org.apache.spark.rdd.RDD[(Int, scala.collection.immutable.Range.Inclusive)] = MapPartitionsRDD[13] at mapValues at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; b.collect</span><br><span class="line">res20: Array[(Int, scala.collection.immutable.Range.Inclusive)] = Array((1,Range 1 to 2), (3,Range 1 to 4), (5,Range 1 to 6))</span><br><span class="line"></span><br><span class="line">// 可使用map实现同样的操作</span><br><span class="line">scala&gt; val b = a.map(x =&gt; (x._1, 1 to x._2))</span><br><span class="line">b: org.apache.spark.rdd.RDD[(Int, scala.collection.immutable.Range.Inclusive)] = MapPartitionsRDD[14] at map at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; b.collect</span><br><span class="line">res21: Array[(Int, scala.collection.immutable.Range.Inclusive)] = Array((1,Range 1 to 2), (3,Range 1 to 4), (5,Range 1 to 6))</span><br><span class="line"></span><br><span class="line">scala&gt; val b = a.map&#123;case (k, v) =&gt; (k, 1 to v)&#125;</span><br><span class="line">b: org.apache.spark.rdd.RDD[(Int, scala.collection.immutable.Range.Inclusive)] = MapPartitionsRDD[15] at map at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; b.collect</span><br><span class="line">res22: Array[(Int, scala.collection.immutable.Range.Inclusive)] = Array((1,Range 1 to 2), (3,Range 1 to 4), (5,Range 1 to 6))</span><br><span class="line"></span><br><span class="line">// flatMapValues 将 value 的值压平</span><br><span class="line">scala&gt; val c = a.flatMapValues(x=&gt;1 to x)</span><br><span class="line">c: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[16] at flatMapValues at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; c.collect</span><br><span class="line">res24: Array[(Int, Int)] = Array((1,1), (1,2), (3,1), (3,2), (3,3), (3,4), (5,1), (5,2), (5,3), (5,4), (5,5), (5,6))</span><br><span class="line"></span><br><span class="line">scala&gt; val c = a.mapValues(x=&gt;1 to x).flatMap&#123;case (k, v) =&gt; v.map(x =&gt; (k, x))&#125;</span><br><span class="line">c: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[18] at flatMap at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; c.collect</span><br><span class="line">res25: Array[(Int, Int)] = Array((1,1), (1,2), (3,1), (3,2), (3,3), (3,4), (5,1), (5,2), (5,3), (5,4), (5,5), (5,6))</span><br><span class="line"></span><br><span class="line">scala&gt; c.keys</span><br><span class="line">res26: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[19] at keys at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt;  c.keys.collect</span><br><span class="line">res29: Array[Int] = Array(1, 1, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5)</span><br><span class="line"></span><br><span class="line">scala&gt; c.values</span><br><span class="line">res27: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[20] at values at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; c.values.collect</span><br><span class="line">res30: Array[Int] = Array(1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6)</span><br><span class="line"></span><br><span class="line">scala&gt; c.map&#123;case (k, v) =&gt; k&#125;.collect</span><br><span class="line">res31: Array[Int] = Array(1, 1, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5)</span><br><span class="line"></span><br><span class="line">scala&gt; c.map&#123;case (k, _) =&gt; k&#125;.collect</span><br><span class="line">res32: Array[Int] = Array(1, 1, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5)</span><br><span class="line"></span><br><span class="line">scala&gt; c.map&#123;case (_, v) =&gt; v&#125;.collect</span><br><span class="line">res33: Array[Int] = Array(1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6)</span><br></pre></td></tr></table></figure>
<h4 id="聚合操作【重要、难点】">聚合操作【重要、难点】</h4>
<p>**PariRDD(k, v)**使用范围广，聚合</p>
<p><strong>groupByKey / reduceByKey / foldByKey / aggregateByKey</strong></p>
<p><strong>combineByKey（OLD） / combineByKeyWithClassTag（NEW） =&gt; 底层实现</strong></p>
<p><strong>subtractByKey</strong>：类似于subtract，删掉 RDD 中键与 other RDD 中的键相同的元素</p>
<p>小案例：给定一组数据：(“spark”, 12), (“hadoop”, 26), (“hadoop”, 23), (“spark”, 15), (“scala”, 26), (“spark”, 25), (“spark”, 23), (“hadoop”, 16), (“scala”, 24), (“spark”, 16)， 键值对的key表示图书名称，value表示某天图书销量。计算每个键对应的平均值，也就是计算每种图书的每天平均销量。</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.makeRDD(Array((&quot;spark&quot;, 12), (&quot;hadoop&quot;, 26), (&quot;hadoop&quot;, 23), (&quot;spark&quot;, 15), (&quot;scala&quot;, 26), (&quot;spark&quot;, 25), (&quot;spark&quot;, 23), (&quot;hadoop&quot;, 16), (&quot;scala&quot;, 24), (&quot;spark&quot;, 16)))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[27] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">// groupByKey 按照key分组</span><br><span class="line">scala&gt; rdd.groupByKey().collect</span><br><span class="line">res45: Array[(String, Iterable[Int])] = Array((hadoop,CompactBuffer(23, 16, 26)), (spark,CompactBuffer(12, 15, 25, 23, 16)), (scala,CompactBuffer(26, 24)))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.groupByKey().map(x=&gt;(x._1, x._2.sum.toDouble/x._2.size)).collect</span><br><span class="line">res34: Array[(String, Double)] = Array((hadoop,21.666666666666668), (spark,18.2), (scala,25.0))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.groupByKey().map&#123;case (k, v) =&gt; (k, v.sum.toDouble/v.size)&#125;.collect</span><br><span class="line">res35: Array[(String, Double)] = Array((hadoop,21.666666666666668), (spark,18.2), (scala,25.0))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.groupByKey.mapValues(v =&gt; v.sum.toDouble/v.size).collect</span><br><span class="line">res36: Array[(String, Double)] = Array((hadoop,21.666666666666668), (spark,18.2), (scala,25.0))</span><br><span class="line"></span><br><span class="line">// reduceByKey 按照key去聚合</span><br><span class="line">scala&gt; rdd.reduceByKey(_+_).collect</span><br><span class="line">res46: Array[(String, Int)] = Array((hadoop,65), (spark,91), (scala,50))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.mapValues((_, 1)).collect</span><br><span class="line">res47: Array[(String, (Int, Int))] = Array((spark,(12,1)), (hadoop,(26,1)), (hadoop,(23,1)), (spark,(15,1)), (scala,(26,1)), (spark,(25,1)), (spark,(23,1)), (hadoop,(16,1)), (scala,(24,1)), (spark,(16,1)))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.mapValues((_, 1)).reduceByKey((x, y)=&gt; (x._1+y._1, x._2+y._2)).collect</span><br><span class="line">res48: Array[(String, (Int, Int))] = Array((hadoop,(65,3)), (spark,(91,5)), (scala,(50,2)))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.mapValues((_, 1)).reduceByKey((x, y)=&gt; (x._1+y._1, x._2+y._2)).mapValues(x =&gt; (x._1.toDouble / x._2)).collect()</span><br><span class="line">res37: Array[(String, Double)] = Array((hadoop,21.666666666666668), (spark,18.2), (scala,25.0))</span><br><span class="line"></span><br><span class="line">// foldByKey 给初值去聚合</span><br><span class="line">scala&gt; rdd.mapValues((_, 1)).foldByKey((0, 0))((x, y) =&gt; &#123; (x._1+y._1, x._2+y._2) &#125;).mapValues(x=&gt;x._1.toDouble/x._2).collect</span><br><span class="line">res38: Array[(String, Double)] = Array((hadoop,21.666666666666668), (spark,18.2), (scala,25.0))</span><br><span class="line"></span><br><span class="line">// aggregateByKey</span><br><span class="line">// aggregateByKey =&gt; 定义初值 + 分区内的聚合函数 + 分区间的聚合函数</span><br><span class="line">scala&gt; rdd.mapValues((_, 1)).aggregateByKey((0,0))( (x, y) =&gt; (x._1 + y._1, x._2 + y._2), (a, b) =&gt; (a._1 + b._1, a._2 + b._2) ).mapValues(x=&gt;x._1.toDouble / x._2).collect</span><br><span class="line">res39: Array[(String, Double)] = Array((hadoop,21.666666666666668), (spark,18.2), (scala,25.0))</span><br><span class="line"></span><br><span class="line">// 初值(元组)与RDD元素类型(Int)可以不一致</span><br><span class="line">scala&gt; rdd.aggregateByKey((0, 0))( (x, y) =&gt; &#123;println(s&quot;x=$x, y=$y&quot;); (x._1 + y, x._2 + 1)&#125;, (a, b) =&gt; &#123;println(s&quot;a=$a, b=$b&quot;); (a._1 + b._1, a._2 + b._2)&#125; ).mapValues(x=&gt;x._1.toDouble/x._2).collect</span><br><span class="line">res40: Array[(String, Double)] = Array((hadoop,21.666666666666668), (spark,18.2), (scala,25.0))</span><br><span class="line"></span><br><span class="line">// 分区内的合并与分区间的合并，可以采用不同的方式；这种方式是低效的！</span><br><span class="line">scala&gt; rdd.aggregateByKey(scala.collection.mutable.ArrayBuffer[Int] ())((x, y) =&gt; &#123;x.append(y); x&#125;, (a, b) =&gt; &#123;a++b&#125; ).mapValues(v =&gt; v.sum.toDouble/v.size).collect</span><br><span class="line">res41: Array[(String, Double)] = Array((hadoop,21.666666666666668), (spark,18.2), (scala,25.0))</span><br><span class="line"></span><br><span class="line">// combineByKey(理解就行)</span><br><span class="line">scala&gt; rdd.combineByKey( (x: Int) =&gt; &#123;println(s&quot;x=$x&quot;); (x,1)&#125;, (x: (Int, Int), y: Int) =&gt; &#123;println(s&quot;x=$x, y=$y&quot;);(x._1+y, x._2+1)&#125;, (a: (Int, Int), b: (Int, Int)) =&gt; &#123;println(s&quot;a=$a, b=$b&quot;); (a._1+b._1, a._2+b._2)&#125; ).mapValues(x=&gt;x._1.toDouble/x._2).collect</span><br><span class="line">res42: Array[(String, Double)] = Array((hadoop,21.666666666666668), (spark,18.2), (scala,25.0))</span><br><span class="line"></span><br><span class="line">// subtractByKey</span><br><span class="line">scala&gt; val rdd1 = sc.makeRDD(Array((&quot;spark&quot;, 12), (&quot;hadoop&quot;, 26), (&quot;hadoop&quot;, 23), (&quot;spark&quot;, 15)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[49] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = sc.makeRDD(Array((&quot;spark&quot;, 100), (&quot;hadoop&quot;, 300)))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[50] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.subtractByKey(rdd2).collect()</span><br><span class="line">res43: Array[(String, Int)] = Array()</span><br><span class="line"></span><br><span class="line">// subtractByKey</span><br><span class="line">scala&gt; val rdd = sc.makeRDD(Array((&quot;a&quot;,1), (&quot;b&quot;,2), (&quot;c&quot;,3), (&quot;a&quot;,5), (&quot;d&quot;,5)))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[52] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val other = sc.makeRDD(Array((&quot;a&quot;,10), (&quot;b&quot;,20), (&quot;c&quot;,30)))</span><br><span class="line">other: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[53] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.subtractByKey(other).collect()</span><br><span class="line">res44: Array[(String, Int)] = Array((d,5))</span><br></pre></td></tr></table></figure>
<p>结论：效率相等用最熟悉的方法；groupByKey在一般情况下效率低，尽量少用</p>
<p>初学：最重要的是实现；如果使用了groupByKey，寻找替换的算子实现；</p>
  <img src="/posts/1276889862/2.jpg" class="post-image">
  <br>
<p><strong>groupByKey Shuffle过程中传输的数据量大，效率低</strong></p>
  <img src="/posts/1276889862/3.jpg" class="post-image">
  <br>
<h4 id="排序操作">排序操作</h4>
<p>sortByKey：sortByKey函数作用于PairRDD，对Key进行排序。在 org.apache.spark.rdd.OrderedRDDFunctions 中实现：</p>
  <img src="/posts/1276889862/4.jpg" class="post-image">
  <br>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(List(&quot;wyp&quot;, &quot;iteblog&quot;, &quot;com&quot;, &quot;397090770&quot;, &quot;test&quot;))</span><br><span class="line">a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[64] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val b = sc.parallelize (1 to a.count.toInt)</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[65] at parallelize at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; val c = a.zip(b)</span><br><span class="line">c: org.apache.spark.rdd.RDD[(String, Int)] = ZippedPartitionsRDD2[66] at zip at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line">scala&gt; c.sortByKey().collect</span><br><span class="line">res51: Array[(String, Int)] = Array((397090770,4), (com,3), (iteblog,2), (test,5), (wyp,1))</span><br><span class="line"></span><br><span class="line">scala&gt; c.sortByKey(false).collect</span><br><span class="line">res52: Array[(String, Int)] = Array((wyp,1), (test,5), (iteblog,2), (com,3), (397090770,4))</span><br></pre></td></tr></table></figure>
<h4 id="join操作">join操作</h4>
<p><strong>cogroup / join / leftOuterJoin / rightOuterJoin / fullOuterJoin</strong></p>
  <img src="/posts/1276889862/5.jpg" class="post-image">
  <br>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.makeRDD(Array((1,&quot;Spark&quot;), (2,&quot;Hadoop&quot;), (3,&quot;Kylin&quot;), (4,&quot;Flink&quot;)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[73] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = sc.makeRDD(Array((3,&quot;李四&quot;), (4,&quot;王五&quot;), (5,&quot;赵六&quot;), (6,&quot;冯七&quot;)))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[74] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd3 = rdd1.cogroup(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.RDD[(Int, (Iterable[String], Iterable[String]))] = MapPartitionsRDD[76] at cogroup at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect.foreach(println)</span><br><span class="line">(1,(CompactBuffer(Spark),CompactBuffer()))</span><br><span class="line">(2,(CompactBuffer(Hadoop),CompactBuffer()))</span><br><span class="line">(3,(CompactBuffer(Kylin),CompactBuffer(李四)))</span><br><span class="line">(4,(CompactBuffer(Flink),CompactBuffer(王五)))</span><br><span class="line">(5,(CompactBuffer(),CompactBuffer(赵六)))</span><br><span class="line">(6,(CompactBuffer(),CompactBuffer(冯七)))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.filter&#123;case (_, (v1, v2)) =&gt; v1.nonEmpty &amp; v2.nonEmpty&#125;.collect</span><br><span class="line">res54: Array[(Int, (Iterable[String], Iterable[String]))] = Array((3,(CompactBuffer(Kylin),CompactBuffer(李四))), (4,(CompactBuffer(Flink),CompactBuffer(王五))))</span><br><span class="line"></span><br><span class="line">// 仿照源码实现join操作</span><br><span class="line">scala&gt; rdd3.flatMapValues( pair =&gt; for (v &lt;- pair._1.iterator; w &lt;- pair._2.iterator) yield (v, w))</span><br><span class="line">res55: org.apache.spark.rdd.RDD[(Int, (String, String))] = MapPartitionsRDD[78] at flatMapValues at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd1 = sc.makeRDD(Array((&quot;1&quot;,&quot;Spark&quot;),(&quot;2&quot;,&quot;Hadoop&quot;), (&quot;3&quot;,&quot;Scala&quot;),(&quot;4&quot;,&quot;Java&quot;)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[79] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = sc.makeRDD(Array((&quot;3&quot;,&quot;20K&quot;),(&quot;4&quot;,&quot;18K&quot;), (&quot;5&quot;,&quot;25K&quot;),(&quot;6&quot;,&quot;10K&quot;)))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[80] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.join(rdd2).collect</span><br><span class="line">res56: Array[(String, (String, String))] = Array((3,(Scala,20K)), (4,(Java,18K)))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.leftOuterJoin(rdd2).collect</span><br><span class="line">res57: Array[(String, (String, Option[String]))] = Array((1,(Spark,None)), (2,(Hadoop,None)), (3,(Scala,Some(20K))), (4,(Java,Some(18K))))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.rightOuterJoin(rdd2).collect</span><br><span class="line">res58: Array[(String, (Option[String], String))] = Array((3,(Some(Scala),20K)), (4,(Some(Java),18K)), (5,(None,25K)), (6,(None,10K)))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.fullOuterJoin(rdd2).collect</span><br><span class="line">res59: Array[(String, (Option[String], Option[String]))] = Array((1,(Some(Spark),None)), (2,(Some(Hadoop),None)), (3,(Some(Scala),Some(20K))), (4,(Some(Java),Some(18K))), (5,(None,Some(25K))), (6,(None,Some(10K))))</span><br></pre></td></tr></table></figure>
<h3 id="Action操作">Action操作</h3>
<p>collectAsMap / countByKey / lookup(key)</p>
<p>countByKey源码：</p>
  <img src="/posts/1276889862/6.jpg" class="post-image">
  <br>
<p>lookup(key)：高效的查找方法，只查找对应分区的数据（如果RDD有分区器的话）</p>
  <img src="/posts/1276889862/7.jpg" class="post-image">
  <br>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;   val rdd1 = sc.makeRDD(Array((&quot;1&quot;,&quot;Spark&quot;),(&quot;2&quot;,&quot;Hadoop&quot;), (&quot;3&quot;,&quot;Scala&quot;),(&quot;1&quot;,&quot;Java&quot;)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[93] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;   val rdd2 = sc.makeRDD(Array((&quot;3&quot;,&quot;20K&quot;),(&quot;4&quot;,&quot;18K&quot;), (&quot;5&quot;,&quot;25K&quot;),(&quot;6&quot;,&quot;10K&quot;)))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[94] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;   rdd1.lookup(&quot;1&quot;)</span><br><span class="line">res60: Seq[String] = WrappedArray(Spark, Java)</span><br><span class="line"></span><br><span class="line">scala&gt;   rdd2.lookup(&quot;3&quot;)</span><br><span class="line">res61: Seq[String] = WrappedArray(20K)</span><br></pre></td></tr></table></figure>
<h2 id="输入与输出-2">输入与输出</h2>
<h3 id="文件输入与输出">文件输入与输出</h3>
<h4 id="文本文件">文本文件</h4>
<p>数据读取：textFile(String)。可指定单个文件，<strong>支持通配符</strong>。</p>
<p>这样对于大量的小文件读取效率并不高，应该使用 <strong>wholeTextFiles</strong></p>
<p>def wholeTextFiles(path: String, minPartitions: Int = defaultMinPartitions):RDD[(String, String)])</p>
<p><strong>返回值RDD[(String, String)]，其中Key是文件的名称，Value是文件的内容</strong></p>
<p>数据保存：saveAsTextFile(String)。指定的输出目录。</p>
<h4 id="csv文件">csv文件</h4>
<p>读取 CSV（Comma-Separated Values）/TSV（Tab-Separated Values） 数据和读取 JSON 数据相似，都需要先把文件当作普通文本文件来读取数据，然后通过将每一行进行解析实现对CSV的读取。</p>
<p>CSV/TSV 数据的输出也是需要将结构化RDD通过相关的库转换成字符串RDD，然后使用 Spark 的文本文件 API 写出去。</p>
<h4 id="json文件-2">json文件</h4>
<p>如果 JSON 文件中每一行就是一个JSON记录，那么可以通过将JSON文件当做文本文件来读取，然后利用相关的JSON库对每一条数据进行JSON解析。</p>
<p>JSON数据的输出主要是通过在输出之前将由结构化数据组成的 RDD 转为字符串RDD，然后使用 Spark 的文本文件 API 写出去。</p>
<p>json文件的处理使用SparkSQL最为简洁。</p>
<h4 id="SequenceFile">SequenceFile</h4>
<p>SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。 Spark 有专门用来读取 SequenceFile 的接口。在 SparkContext中，可以调用：sequenceFile[keyClass, valueClass]；</p>
<p>调用 saveAsSequenceFile(path) 保存PairRDD，系统将键和值能够自动转为Writable类型。</p>
<h4 id="对象文件">对象文件</h4>
<p>对象文件是将对象序列化后保存的文件，采用Java的序列化机制。</p>
<p>通过 objectFile<a href="path">k,v</a> 接收一个路径，读取对象文件，返回对应的 RDD，也可以通过调用saveAsObjectFile() 实现对对象文件的输出。因为是序列化所以要指定类型。</p>
<h4 id="JDBC-2">JDBC</h4>
<p>详见Scala算子综合应用案例博客文档</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://raoweijiapng.github.io">WeiJia Rao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://raoweijiapng.github.io/posts/1276889862/">https://raoweijiapng.github.io/posts/1276889862/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://raoweijiapng.github.io" target="_blank">WeiJia_Rao</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/RDD/">RDD</a><a class="post-meta__tags" href="/tags/Spark-Core/">Spark Core</a></div><div class="post_share"><div class="social-share" data-image="/img/1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/3590682603/"><img class="prev-cover" src="/img/2.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">RDD编程之基本使用</div></div></a></div><div class="next-post pull-right"><a href="/posts/667651910/"><img class="next-cover" src="/img/4.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">RDD算子综合应用案例</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/667651910/" title="RDD算子综合应用案例"><img class="cover" src="/img/4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-26</div><div class="title">RDD算子综合应用案例</div></div></a></div><div><a href="/posts/3379543885/" title="RDD编程之分区与分区器"><img class="cover" src="/img/2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-30</div><div class="title">RDD编程之分区与分区器</div></div></a></div><div><a href="/posts/3590682603/" title="RDD编程之基本使用"><img class="cover" src="/img/2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-25</div><div class="title">RDD编程之基本使用</div></div></a></div><div><a href="/posts/4167654689/" title="RDD编程之基本概念"><img class="cover" src="/img/7.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-25</div><div class="title">RDD编程之基本概念</div></div></a></div><div><a href="/posts/141009478/" title="RDD编程之广播变量和累加器以及TopN的优化"><img class="cover" src="/img/6.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-30</div><div class="title">RDD编程之广播变量和累加器以及TopN的优化</div></div></a></div><div><a href="/posts/3831775535/" title="RDD编程之序列化与依赖关系"><img class="cover" src="/img/5.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-29</div><div class="title">RDD编程之序列化与依赖关系</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/authorPhoto.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">WeiJia Rao</div><div class="author-info__description">饶唯甲的个人博客网站,用于记录平时的学习笔记并展示。努力学习吧,少年!</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">422</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">70</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">59</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://gitee.com/raoweijiapng"><i></i><span>My Gitee</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="mailto:raoweijia@outlook.com" target="_blank" title="邮箱"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://space.bilibili.com/217971296?spm_id_from=333.1007.0.0" target="_blank" title="哔哩哔哩"><i class="fa-brands fa-bilibili"></i></a><a class="social-icon" href="https://weibo.com/p/1005057628848053" target="_blank" title="微博"><i class="fa-brands fa-weibo"></i></a><a class="social-icon" href="https://www.zhihu.com/people/kan-kan-ni-66-95" target="_blank" title="知乎"><i class="fa-brands fa-zhihu"></i></a><a class="social-icon" href="https://github.com/raoweijiapng" target="_blank" title="My-GitHub"><i class="fa-brands fa-github"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Key-Value-RDD%E6%93%8D%E4%BD%9C"><span class="toc-number">1.</span> <span class="toc-text">Key-Value RDD操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAPair-RDD"><span class="toc-number">1.1.</span> <span class="toc-text">创建Pair RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformation%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.</span> <span class="toc-text">Transformation操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B1%BB%E4%BC%BC-map-%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.1.</span> <span class="toc-text">类似 map 操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%81%9A%E5%90%88%E6%93%8D%E4%BD%9C%E3%80%90%E9%87%8D%E8%A6%81%E3%80%81%E9%9A%BE%E7%82%B9%E3%80%91"><span class="toc-number">1.2.2.</span> <span class="toc-text">聚合操作【重要、难点】</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%92%E5%BA%8F%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.3.</span> <span class="toc-text">排序操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#join%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.4.</span> <span class="toc-text">join操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Action%E6%93%8D%E4%BD%9C"><span class="toc-number">1.3.</span> <span class="toc-text">Action操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BA-2"><span class="toc-number">2.</span> <span class="toc-text">输入与输出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BA"><span class="toc-number">2.1.</span> <span class="toc-text">文件输入与输出</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6"><span class="toc-number">2.1.1.</span> <span class="toc-text">文本文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#csv%E6%96%87%E4%BB%B6"><span class="toc-number">2.1.2.</span> <span class="toc-text">csv文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#json%E6%96%87%E4%BB%B6-2"><span class="toc-number">2.1.3.</span> <span class="toc-text">json文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SequenceFile"><span class="toc-number">2.1.4.</span> <span class="toc-text">SequenceFile</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E8%B1%A1%E6%96%87%E4%BB%B6"><span class="toc-number">2.1.5.</span> <span class="toc-text">对象文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#JDBC-2"><span class="toc-number">2.1.6.</span> <span class="toc-text">JDBC</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/1964138807/" title="第八章-项目整合管理">第八章-项目整合管理</a><time datetime="2023-08-17T07:55:58.000Z" title="发表于 2023-08-17 15:55:58">2023-08-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/2958033879/" title="第七章-项目立项管理">第七章-项目立项管理</a><time datetime="2023-08-17T03:36:12.000Z" title="发表于 2023-08-17 11:36:12">2023-08-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/2602906239/" title="第六章-项目管理概论">第六章-项目管理概论</a><time datetime="2023-08-16T09:34:44.000Z" title="发表于 2023-08-16 17:34:44">2023-08-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/3452200185/" title="第五章-信息系统工程">第五章-信息系统工程</a><time datetime="2023-08-15T09:38:42.000Z" title="发表于 2023-08-15 17:38:42">2023-08-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/2814949322/" title="第四章-信息系统管理">第四章-信息系统管理</a><time datetime="2023-08-14T01:17:33.000Z" title="发表于 2023-08-14 09:17:33">2023-08-14</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/footer_img.png')"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By WeiJia Rao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://raoweijiapng.gitee.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><div class="aplayer no-destroy" data-id="60198" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="false"> </div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script>(function(d, w, c) {
    w.ChatraID = '5QYmoz7m5kBKqo6Hi';
    var s = d.createElement('script');
    w[c] = w[c] || function() {
        (w[c].q = w[c].q || []).push(arguments);
    };
    s.async = true;
    s.src = 'https://call.chatra.io/chatra.js';
    if (d.head) d.head.appendChild(s);
})(document, window, 'Chatra');

if (false) {
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      Chatra('openChat')
    });
  }
  chatBtnFn()
} else {
  if (false) {
    function chatBtnHide () {
      Chatra('hide')
    }
    function chatBtnShow () {
      Chatra('show')
    }
  }
}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>